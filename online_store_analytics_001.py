# -*- coding: utf-8 -*-
"""Online Store Analytics - 001.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1psj_tcoMf_WVPC6fFrOA8szcCS5xWoHQ

# **1. INTRODUCTION**

## **a. Description and Objectives**

The dataset contains records of retail transanctions. It's made up of 8 columns including InvoiceNo, StockCode, Description, Quantity, InvoiceDate, UnitPrice, CustomerID and Country. The dataset was derived from Kaggle and below are the objectives of the analysis;
1. To identify products that drive most revenue
2. Which customers are the most valuable for the company
3. Which countries contribute the most revenue
4. Seasonal patterns in sales
5. Which products are frequently returned
6. How to target customers for marketing

## **b. Libraries and Dataset Upload**
"""

# import libraries

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')
pd.options.display.float_format = '{:,.2f}'.format

#upload the dataset
file_path = '/content/drive/MyDrive/Online strore Dataset.csv'
df = pd.read_csv(file_path)

"""# **2. EYEBALLING**"""

# data overview
df.sample(10)

df.shape

#dataset characteristics
df.info()

# Rows and Columns
rows = df.shape[0]
columns = df.shape[1]
print(f'rows: {rows}, \n columns: {columns}')

#check missing values
df.isnull().sum()

df = df.dropna()

"""# **3. DATA CLEANING AND PREPROCESSING**"""

df.isna().sum()

# new data shape
df.shape

# info
df.info()

#convert customerid to string
df['CustomerID'] = df['CustomerID'].astype(int)
df['CustomerID'] = df['CustomerID'].astype(str)

# invoicedate to datetime
df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'], dayfirst=True, errors='coerce')

df.info()



df['CustomerID'].nunique()

# Calculate revenue
df['Revenue'] = df['Quantity']*df['UnitPrice']

df.sample(10)

# extract hour
df['Hour'] = df['InvoiceDate'].dt.hour
df['Day'] = df['InvoiceDate'].dt.day_name()
df['Month'] = df['InvoiceDate'].dt.month_name()
df['Year'] = df['InvoiceDate'].dt.year
df['Quarter'] = df['InvoiceDate'].dt.quarter
df['Invoice_Date'] = df['InvoiceDate'].dt.date

#create year_month column
df['Year_month'] = df['InvoiceDate'].dt.to_period('M')

df['Country'].unique()

df['Country'] = df['Country'].replace('EIRE', 'Ireland')

#map quarters
df['Quarter'] = df['Quarter'].map({1:'Q1', 2:'Q2', 3:'Q3', 4:'Q4'})

# Create time column
bins = [0, 11, 16, 23]
labels = ['Morning', 'Afternoon', 'Evening']
df['Time'] = pd.cut(df['Hour'], bins=bins, labels=labels)

df.sample(3)

# Check and treat outliers
df[df['Quantity']>3000]

# treat outliers for quantity
quant = df['Quantity']

#calculate IQR
Q1 = quant.quantile(0.25)
Q3 = quant.quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5*IQR
upper_bound = Q3 + 1.5*IQR
outliers = (quant<lower_bound)|(quant >upper_bound)



# treat
df['Quantity'] = quant.clip(lower= lower_bound, upper=upper_bound)

# box plot for quantity
df['Quantity'].plot(kind='box')

# treat unit price
unit_pric = df['UnitPrice']
#calculate IQR
Q1 = unit_pric.quantile(0.25)
Q3 = unit_pric.quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5*IQR
upper_bound = Q3 + 1.5*IQR
outliers = (unit_pric<lower_bound)|(unit_pric >upper_bound)


#treat
df['UnitPrice'] = unit_pric.clip(lower=lower_bound, upper=upper_bound)

# unit price box plot
df['UnitPrice'].plot(kind='box')

# Calculate revenue (sterling pound)
df['Revenue'] = df['Quantity']*df['UnitPrice']

"""# **4. EXPLORATORY DATA ANALYSIS (EDA)**

## a. Descriptive Statistics
"""

# Descriptive stats
df.describe()

"""**Notes**
- Quantity averages 8 items
- Negative quantities represents items returned by the customers
- The maximum quantity per line is 27 items
- The mean of Unit Price is £2.6, meaning most products are priced under £4

## b. Revenue Trends
"""

# summary stats
df['Revenue'].describe()

"""**Notes**
- Average revenue per transaction is £14.46
- Negative values represent equivalent value of sales returns
- 25% of the transactions generates £4.2 or less in revenue
75% of the transactions generated £17.7 or less in revenue
- The maximum revenue was £202.5
- Coefficient of variation (17.89/14.46) is 1.24 indicating high variability in revenue
"""

# revenue across years

df.groupby(['Year', 'Month'])['Revenue'].agg('sum').reset_index().sort_values(['Year', 'Revenue'])

"""**Notes**
- Revenue distributed in two years,from Dec 2010  to Nov 2011
- November was the best performing month with revenue of £853,395.11
- December 2011 was the lowest performing month followed by February
"""

# total revenue by quarter
df.groupby(['Year', 'Quarter'])['Revenue'].agg('sum').reset_index().sort_values(by='Revenue')

df.groupby(['Quarter'])['Revenue'].agg('sum').reset_index().sort_values(by='Revenue')

"""**Notes**
- Q4 was the best performing quarter in 2011 with a total revenue of £1,781,937
- Q1 was generated the lowest total revenue with £1,039,544

"""

# Revenue by time of the day
df.groupby('Time')['Revenue'].agg('sum').reset_index().sort_values(by='Revenue')

"""**Notes**
- Most revenue is generated in the afternoon indicating that most customers make their online transactions in the afternoon
- Least revenue is generated in the evenings
"""

df.sample(3)

"""## **c. Product Analysis**"""

# 10 ten best performing products categories
df.groupby('Description')['Revenue'].agg('sum').reset_index().sort_values(by='Revenue', ascending=True)[-10:]



"""## d. Geographic Trends"""

# revenue by country
df.groupby('Country')['Revenue'].agg('sum').reset_index().sort_values(by='Revenue', ascending=False)

"""# **5. RFM ANALYSIS**"""

df = df[~df['InvoiceNo'].astype(str).str.startswith('C')]

df.head(2)

# Recency
analysis_date = df['InvoiceDate'].max()
analysis_date

last_tran_date = df.groupby('CustomerID')['InvoiceDate'].max()
recency = (analysis_date-last_tran_date).dt.days.reset_index()
recency.rename(columns={'InvoiceDate': 'Recency'}, inplace=True)
recency.sample(4)

# Monetary
monetary = df.groupby('CustomerID', as_index=False)['Revenue'].agg('sum')
monetary.rename(columns={'Revenue':'Monetary'}, inplace=True)
monetary.sample(4)

#frequency
frequency = df.groupby('CustomerID', as_index=False)['InvoiceNo'].nunique()
frequency.rename(columns={'InvoiceNo': 'Frequency'}, inplace=True)
frequency.sample(3)

#merge dataframes
rfm = recency.merge(monetary, on='CustomerID').merge(frequency, on='CustomerID')
rfm.sample(3)

# create bins and labels
rfm['Recency_score'] = pd.qcut(rfm['Recency'].rank(method='first'), q=5, labels=[5,4,3,2,1])
rfm['Monetary_score'] = pd.qcut(rfm['Monetary'].rank(method='first'), q=5, labels=[1,2,3,4,5])
rfm['Frequency_score'] = pd.qcut(rfm['Frequency'].rank(method='first'), q=5, labels=[1,2,3,4,5])
rfm['Score'] = rfm['Recency_score'].astype(str) + rfm['Monetary_score'].astype(str) + rfm['Frequency_score'].astype(str)

rfm.sample(3)

# lets write a short function to elaborate the score further by segmentation
def segment_customers (row):
  if row['Score']=='555':
    return 'Premium customers'
  elif row['Frequency_score']>=4:
    return 'Loyal customers'
  elif row['Monetary_score']>=4:
    return 'Heavy transactors'
  elif row['Recency_score']>=3:
    return 'Recent Customers'
  elif row['Recency_score']<=2:
    return 'Low engagement customers'
  elif row['Recency_score']<=3 and row['Frequency_score']<=2:
    return 'Dormant customers'
  elif row['Recency_score']>=4 and row['Frequency_score']>=3:
    return 'Active Customers'
  else:
    return 'Lost customers'

# create a segment column then apply
rfm['Segment'] = rfm.apply(segment_customers, axis=1)
rfm.sample(3)

# sample top customers
rfm[rfm['Segment']=='Premium customers'][:10].sort_values(by='Monetary', ascending=False)

# Visualize the rfm score results of frequency against monetary
plt.figure(figsize=(8,6))
sns.scatterplot(data=rfm, x='Recency', y='Frequency', hue='Segment')
plt.title('ScatterPlot of Frequency  vs Recency')
plt.xlabel('Recency')
plt.ylabel('Frequency')
plt.show()

"""# **6. COHORT ANALYSIS**"""

# create transaction month
#df['TransactionMonth'] =df['TransactionDate'].dt.to_period('M')
cohort = df.groupby('CustomerID')['Year_month'].min().reset_index()
cohort.rename(columns={'Year_month':'CohortMonth'}, inplace=True)
cohort.sample(4)

# merge the dataframe on the main dataframe
df = df.merge(cohort, on = 'CustomerID')
df.head(2)

#calculating retention

#group by both cohort month and transaction month
cohort_df = df.groupby(['CohortMonth', 'Year_month']).agg(n_customer=('CustomerID','nunique')).reset_index()
cohort_df.head(5)

#group the cohort_df by cohort month and pick the first
cohort_size = cohort_df.groupby('CohortMonth')['n_customer'].first().reset_index()
cohort_size.rename(columns={'n_customer': 'Cohort_size'}, inplace=True)

#merge with cohort_df
cohort_df = cohort_df.merge(cohort_size, on='CohortMonth')
cohort_df

#create new column called transaction rate
cohort_df['Transaction_Rate'] = cohort_df['n_customer']/ cohort_df['Cohort_size']

#del cohort_df['Retention_Rate']
cohort_df

#create a purchase matrix by use of a pivot table

purchase_matrix = cohort_df.pivot(
                              index = 'CohortMonth',
                              columns = 'Year_month',
                              values = 'Transaction_Rate'


)
purchase_matrix

# use heatmap to visualize the matrix

plt.figure(figsize=(8,6))
sns.heatmap(purchase_matrix, annot = True, cmap='coolwarm', fmt='.0%')
plt.title('Heatmap of Customer Purchase Rate')
plt.show()

df.to_csv('OnlineStore.csv', index=False)